{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproducability\n",
    "from numpy.random import seed\n",
    "seed(1+347823)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1+63493)\n",
    "\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_RM_GW_and_HYRAS_Data(ID):\n",
    "    pathGW = \"./01_GWdata/\"\n",
    "    pathHYRAS = \"./00_HYRAS/\"\n",
    "\n",
    "    GWData = pd.read_csv(pathGW+ID+'_GW-Data.csv', \n",
    "                          parse_dates=['Date'],index_col=0, dayfirst = True, \n",
    "                          decimal = '.', sep=',')\n",
    "    HYRASData = pd.read_csv(pathHYRAS+ID+'_Hyras_weekly.csv',\n",
    "                            parse_dates=['Date'],index_col=0, dayfirst = True,\n",
    "                            decimal = '.', sep=',')\n",
    "    data = pd.merge(GWData, HYRASData, how='inner', left_index = True, right_index = True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(data, GLOBAL_SETTINGS):\n",
    "    '''\n",
    "    split data in four parts: training, early stopping, optimization, and testing\n",
    "    strict dates for testing and optimization, relative portions for training and early stopping of the rest\n",
    "    according to the input sequence length the parts do overlap (\"_ext\"), so the model can make use of the full data\n",
    "    '''\n",
    "    dataset = data[(data.index < GLOBAL_SETTINGS[\"test_start\"])]\n",
    "    dataset2 = data[(data.index < GLOBAL_SETTINGS[\"opt_start\"])]\n",
    "    \n",
    "    TrainingData = dataset2[0:round(0.9 * len(dataset2))]\n",
    "    StopData = dataset2[round(0.9 * len(dataset2))+1:]\n",
    "    StopData_ext = dataset2[round(0.9 * len(dataset2))+1-GLOBAL_SETTINGS[\"seq_length\"]:] \n",
    "    OptData = data[(data.index >= GLOBAL_SETTINGS[\"opt_start\"]) & (data.index < GLOBAL_SETTINGS[\"test_start\"])] \n",
    "    OptData_ext = pd.concat([dataset2.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], OptData], axis=0)                                             \n",
    "\n",
    "    TestData = data[(data.index >= GLOBAL_SETTINGS[\"test_start\"]) & (data.index <= GLOBAL_SETTINGS[\"test_end\"])] \n",
    "    TestData_ext = pd.concat([dataset.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], TestData], axis=0)                                             \n",
    "\n",
    "    return TrainingData, StopData, StopData_ext, OptData, OptData_ext, TestData, TestData_ext\n",
    "\n",
    "\n",
    "def to_supervised(data, GLOBAL_SETTINGS):\n",
    "    '''establish sequence to value data format, function based on code from machinelearningmastery.com'''\n",
    "    X, Y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(data)):\n",
    "        # find the end of this pattern\n",
    "        end_idx = i + GLOBAL_SETTINGS[\"seq_length\"]\n",
    "        # check if we are beyond the dataset\n",
    "        if end_idx >= len(data):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = data[i:end_idx, 1:], data[end_idx, 0]\n",
    "        X.append(seq_x)\n",
    "        Y.append(seq_y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "def gwmodel(ini,GLOBAL_SETTINGS,X_train, Y_train,X_stop, Y_stop):\n",
    "    '''build and train tensorflow model'''\n",
    "    seed(ini+42)\n",
    "    tf.random.set_seed(ini+42)\n",
    "    inp = tf.keras.Input(shape=(GLOBAL_SETTINGS[\"seq_length\"], X_train.shape[2]))\n",
    "    cnn = tf.keras.layers.Conv1D(filters=GLOBAL_SETTINGS[\"filters\"],\n",
    "                                         kernel_size=GLOBAL_SETTINGS[\"kernel_size\"],\n",
    "                                         activation='relu',\n",
    "                                         padding='same')(inp)\n",
    "    cnn = tf.keras.layers.MaxPool1D(padding='same')(cnn)\n",
    "    cnn = tf.keras.layers.Dropout(0.1)(cnn)\n",
    "    cnn = tf.keras.layers.Flatten()(cnn)\n",
    "    cnn = tf.keras.layers.Dense(GLOBAL_SETTINGS[\"dense_size\"], activation='relu')(cnn)\n",
    "    output1 = tf.keras.layers.Dense(1, activation='linear')(cnn)\n",
    "    \n",
    "    # tie together\n",
    "    model = tf.keras.Model(inputs=inp, outputs=output1)\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=GLOBAL_SETTINGS[\"learning_rate\"], epsilon=10E-3, clipnorm=GLOBAL_SETTINGS[\"clip_norm\"])\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    \n",
    "    # early stopping\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, \n",
    "                                          patience=GLOBAL_SETTINGS['patience'],\n",
    "                                          restore_best_weights = True)\n",
    "    \n",
    "    # fit network\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_stop, Y_stop), epochs=GLOBAL_SETTINGS[\"epochs\"],\n",
    "                        verbose=0,batch_size=GLOBAL_SETTINGS[\"batch_size\"], callbacks=[es])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bayesOpt_function(densesize, batchsize, filters):\n",
    "    '''function that is called by the bayesian optimization module and which is repeated over and over again to determine \n",
    "     the performance depending on the hyperparameters '''\n",
    "    \n",
    "    print(ID+\" (pp:{}) BayesOpt-Iteration {}\".format(loc,len(optimizer.res)+1))\n",
    "    \n",
    "    #some HPs allow only int values\n",
    "    densesize = 2**int(densesize)\n",
    "    batchsize = 2**int(batchsize)\n",
    "    filters = 2**int(filters)\n",
    "    \n",
    "    GLOBAL_SETTINGS['batch_size'] = batchsize\n",
    "    GLOBAL_SETTINGS['dense_size'] = densesize\n",
    "    GLOBAL_SETTINGS['filters'] = filters\n",
    "    print('  densesize {}, batchsize {}, filters {}'.format(densesize,batchsize,filters))\n",
    "\n",
    "    ## load data\n",
    "    data = load_RM_GW_and_HYRAS_Data(ID)\n",
    "    \n",
    "    #scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl.fit(pd.DataFrame(data['GWL']))\n",
    "    data_n = pd.DataFrame(scaler.fit_transform(data), index=data.index, columns=data.columns)\n",
    "\n",
    "    #split data\n",
    "    TrainingData, StopData, StopData_ext, OptData, OptData_ext, TestData, TestData_ext = split_data(data, GLOBAL_SETTINGS)\n",
    "    TrainingData_n, StopData_n, StopData_ext_n, OptData_n, OptData_ext_n, TestData_n, TestData_ext_n = split_data(data_n, GLOBAL_SETTINGS)\n",
    "    \n",
    "    #sequence data\n",
    "    X_train, Y_train = to_supervised(TrainingData_n.values, GLOBAL_SETTINGS)\n",
    "    X_stop, Y_stop = to_supervised(StopData_ext_n.values, GLOBAL_SETTINGS) \n",
    "    X_opt, Y_opt = to_supervised(OptData_ext_n.values, GLOBAL_SETTINGS)\n",
    "    X_test, Y_test = to_supervised(TestData_ext_n.values, GLOBAL_SETTINGS) \n",
    "\n",
    "    #build and train model with inimax different initializations\n",
    "    \n",
    "    inimax = 5\n",
    "    optresults_members = np.zeros((len(X_opt), inimax))\n",
    "    for ini in range(inimax):\n",
    "        print('    Member: '+str(ini))\n",
    "        model,history = gwmodel(ini,GLOBAL_SETTINGS,X_train, Y_train, X_stop, Y_stop)  \n",
    "        opt_sim_n = model.predict(X_opt)\n",
    "        opt_sim = scaler_gwl.inverse_transform(opt_sim_n)\n",
    "        optresults_members[:, ini] = opt_sim.reshape(-1,)\n",
    "        \n",
    "        fig = pyplot.figure(figsize=(5,1))\n",
    "        pyplot.plot(history.history['loss'])\n",
    "        pyplot.plot(history.history['val_loss'])\n",
    "        pyplot.title(ID)\n",
    "        pyplot.show()\n",
    "        \n",
    "    # calculate performance\n",
    "    opt_sim_mean = np.mean(optresults_members,axis = 1)    \n",
    "    sim = np.asarray(opt_sim_mean.reshape(-1,1))\n",
    "    obs = np.asarray(scaler_gwl.inverse_transform(Y_opt.reshape(-1,1)))\n",
    "    err = sim-obs\n",
    "    meanTrainingGWL = np.mean(np.asarray(TrainingData['GWL']))\n",
    "    meanStopGWL = np.mean(np.asarray(StopData['GWL']))\n",
    "    err_nash = obs - np.mean([meanTrainingGWL, meanStopGWL])\n",
    "    \n",
    "    try:\n",
    "        r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "        r = r[0] #r\n",
    "    except:\n",
    "        r = [np.nan, np.nan]\n",
    "        r = r[0] #r\n",
    "    NSE = (1 - ((np.sum(err ** 2)) / (np.sum((err_nash) ** 2))))\n",
    "    # score = NSE + r\n",
    "   \n",
    "    #plot optimization result\n",
    "    pyplot.figure(figsize=(7,2))\n",
    "    pyplot.plot(obs,color='k',linewidth=1)\n",
    "    for ini in range(inimax):\n",
    "        pyplot.plot(optresults_members[:,ini],color='r',linewidth=0.5,alpha = 0.5)\n",
    "    pyplot.plot(sim,color='r',linewidth=1)\n",
    "    pyplot.title(ID+', NSE '+str(np.round(NSE,3))+', r '+str(np.round(r,3)))\n",
    "    pyplot.show()\n",
    "    \n",
    "    score = np.mean(err**2)\n",
    "    print(\"score = \"+str(np.round(score,4)))\n",
    "    return -score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_testset():\n",
    "    '''mostly similar to the bayesOpt_function() function,\n",
    "      but used to take the best model and calculate errors on \n",
    "      the test set instead of the optimization set'''\n",
    "    \n",
    "    ## load data\n",
    "    data = load_RM_GW_and_HYRAS_Data(ID)\n",
    "    \n",
    "    #scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl.fit(pd.DataFrame(data['GWL']))\n",
    "    data_n = pd.DataFrame(scaler.fit_transform(data), index=data.index, columns=data.columns)\n",
    "\n",
    "    #split data\n",
    "    TrainingData, StopData, StopData_ext, OptData, OptData_ext, TestData, TestData_ext = split_data(data, GLOBAL_SETTINGS)\n",
    "    TrainingData_n, StopData_n, StopData_ext_n, OptData_n, OptData_ext_n, TestData_n, TestData_ext_n = split_data(data_n, GLOBAL_SETTINGS)\n",
    "    \n",
    "    #sequence data\n",
    "    X_train, Y_train = to_supervised(TrainingData_n.values, GLOBAL_SETTINGS)\n",
    "    X_stop, Y_stop = to_supervised(StopData_ext_n.values, GLOBAL_SETTINGS) \n",
    "    X_opt, Y_opt = to_supervised(OptData_ext_n.values, GLOBAL_SETTINGS)\n",
    "    X_test, Y_test = to_supervised(TestData_ext_n.values, GLOBAL_SETTINGS) \n",
    "\n",
    "    #build and train model with inimax different initializations\n",
    "    inimax = 20\n",
    "    sim_members = np.zeros((len(X_test), inimax))\n",
    "    sim_members[:] = np.nan\n",
    "    \n",
    "    model_path = './Results_XAI/models/'+ID+\"/\" # to save the model\n",
    "    \n",
    "    f = open('./Results_XAI/traininghistory_CNN_'+ID+'.txt', \"w\") # to save additional information on the training permanently\n",
    "    if os.path.isdir(model_path) == False:\n",
    "            os.mkdir(model_path)\n",
    "\n",
    "    fig, axs = pyplot.subplots(inimax,1, figsize = (7,24))\n",
    "    for ini in range(inimax):\n",
    "        model,history = gwmodel(ini,GLOBAL_SETTINGS,X_train, Y_train, X_stop, Y_stop)  \n",
    "        model.save(model_path+str(ini))\n",
    "        loss = np.zeros((1, GLOBAL_SETTINGS['epochs']))\n",
    "        loss[:,:] = np.nan\n",
    "        loss[0,0:np.shape(history.history['loss'])[0]] = history.history['loss']\n",
    "        val_loss = np.zeros((1, GLOBAL_SETTINGS['epochs']))\n",
    "        val_loss[:,:] = np.nan\n",
    "        val_loss[0,0:np.shape(history.history['val_loss'])[0]] = history.history['val_loss']\n",
    "        print('loss', file = f)\n",
    "        print(loss.tolist(), file = f)\n",
    "        print('val_loss', file = f)\n",
    "        print(val_loss.tolist(), file = f)\n",
    "        \n",
    "        axs[ini].plot(history.history['loss'],label = 'training')\n",
    "        axs[ini].plot(history.history['val_loss'],label = 'validation')\n",
    "        axs[ini].title.set_text('ini '+str(ini))\n",
    "\n",
    "        test_sim = model.predict(X_test)\n",
    "        test_sim = scaler_gwl.inverse_transform(test_sim)\n",
    "        sim_members[:,ini]=test_sim.reshape(-1,)\n",
    "    f.close()\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig('./Results_XAI/loss_'+ID+'.png')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # use this part to apply already existing models (need to comment the training and saving part above)\n",
    "    # for ini in range(inimax):    \n",
    "    #     model = tf.keras.models.load_model(model_path+str(ini))\n",
    "    #     test_sim = model.predict(X_test)\n",
    "    #     test_sim = scaler_gwl.inverse_transform(test_sim)\n",
    "    #     sim_members[:,ini]= test_sim.reshape(-1,)\n",
    "    \n",
    "    #plot results\n",
    "    fig, axs = pyplot.subplots(10,2, figsize = (14,24))\n",
    "    col = 0\n",
    "    for ini in range(inimax):\n",
    "        row = ini\n",
    "        if ini > 9:\n",
    "            col = 1\n",
    "            row = ini-10\n",
    "        sim = sim_members[:,ini]\n",
    "        axs[row,col].plot(TestData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "        axs[row,col].plot(TestData.index, np.asarray(scaler_gwl.inverse_transform(Y_test.reshape(-1,1))),\n",
    "                    'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "        axs[row,col].title.set_text(\"Testfit, ini\"+str(ini))\n",
    "        axs[row,col].grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig('./Results_XAI/Members_test_'+ID+'.png')\n",
    "    pyplot.show()        \n",
    "        \n",
    "    sim_mean = np.nanmean(sim_members,axis = 1)\n",
    "    sim_uncertainty = [np.quantile(sim_members, 0.05, axis=1),np.quantile(sim_members, 0.95, axis=1)]\n",
    "    \n",
    "    #introduce GWL t-1 as additional Input (nalso needed for persistency index calculation)\n",
    "    GWData_shift1 = pd.DataFrame(data['GWL'])\n",
    "    GWData_shift1.index = GWData_shift1.index.shift(periods = 7, freq = 'D')\n",
    "    GWData_shift1.rename(columns={\"GWL\": \"GWLt-1\"},inplace=True)\n",
    "    PIData = GWData_shift1[(GWData_shift1.index >= GLOBAL_SETTINGS[\"test_start\"]) & (GWData_shift1.index <= GLOBAL_SETTINGS[\"test_end\"])]\n",
    "    \n",
    "    # get scores\n",
    "    sim = np.asarray(sim_mean.reshape(-1,1))\n",
    "    obs = np.asarray(scaler_gwl.inverse_transform(Y_test.reshape(-1,1)))\n",
    "    obs_PI = np.asarray(PIData).reshape(-1,1)\n",
    "    err = sim-obs\n",
    "    err_rel = (sim-obs)/(np.max(data['GWL'])-np.min(data['GWL']))\n",
    "    err_nash = obs - np.mean(np.asarray(data['GWL'][(data.index < GLOBAL_SETTINGS[\"test_start\"])]))\n",
    "    err_PI = obs-obs_PI\n",
    "    \n",
    "    NSE = 1 - ((np.sum(err ** 2)) / (np.sum((err_nash) ** 2)))\n",
    "    try:\n",
    "        r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "        r = r[0] #r\n",
    "    except:\n",
    "        r = [np.nan, np.nan]\n",
    "        r = r[0] #r\n",
    "    R2 = r ** 2\n",
    "    RMSE =  np.sqrt(np.mean(err ** 2))\n",
    "    rRMSE = np.sqrt(np.mean(err_rel ** 2)) * 100\n",
    "    Bias = np.mean(err)\n",
    "    rBias = np.mean(err_rel) * 100\n",
    "    PI = 1 - ((np.sum(err ** 2)) / (np.sum((err_PI) ** 2)))\n",
    "    alpha = np.std(sim)/np.std(obs)\n",
    "    beta = np.mean(sim)/np.mean(obs)\n",
    "    KGE = 1-np.sqrt((r-1)**2+(alpha-1)**2+(beta-1)**2) #KGE\n",
    "    \n",
    "    scores = pd.DataFrame(np.array([[NSE, KGE, R2, RMSE, rRMSE, Bias, rBias, PI]]),\n",
    "                   columns=['NSE', 'KGE','R2','RMSE','rRMSE','Bias','rBias','PI'])\n",
    "    print(scores)\n",
    "    \n",
    "    return scores, TestData, sim, obs, sim_uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class newJSONLogger(JSONLogger) :\n",
    "      '''enables that the logger continues an already existing file, wich\n",
    "        is also possible in a different way using the lates releases of \n",
    "        the bayesian optimization library (see documentation).\n",
    "      '''\n",
    "      \n",
    "      def __init__(self, path):\n",
    "            self._path=None\n",
    "            super(JSONLogger, self).__init__()\n",
    "            self._path = path if path[-5:] == \".json\" else path + \".json\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "well_list = pd.read_csv(\"./locations.csv\",sep=';',header = 0,encoding = 'Latin1')\n",
    "    \n",
    "for loc in [0]:#range(well_list.shape[0]): # loop all locations\n",
    "    \n",
    "    seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    \n",
    "    ID = well_list.alias[loc]\n",
    "    data = load_RM_GW_and_HYRAS_Data(ID)\n",
    "    \n",
    "    #define hyperparameters and other important stuff    \n",
    "    GLOBAL_SETTINGS = {\n",
    "        'clip_norm': True,\n",
    "        'epochs': 3, #max number of training epochs to allow\n",
    "        'patience': 30, # early stopping patience\n",
    "        'learning_rate': 1e-3,\n",
    "        'seq_length': 52, #length of the data input sequence for the model (here: 52 weeks = 1 year)\n",
    "        'kernel_size': 3,\n",
    "        'opt_start': pd.to_datetime('01012015', format='%d%m%Y'), \n",
    "        'test_start': pd.to_datetime('01012017', format='%d%m%Y'),\n",
    "        'test_end': pd.to_datetime('31122020', format='%d%m%Y')\n",
    "    }\n",
    "    \n",
    "    # if not all locations have data until the \"test_end\" data, you may need something like this\n",
    "    if GLOBAL_SETTINGS[\"test_end\"] > data.index[-1]:\n",
    "        GLOBAL_SETTINGS[\"test_end\"] = data.index[-1]\n",
    "        GLOBAL_SETTINGS[\"test_start\"] = GLOBAL_SETTINGS[\"test_end\"] - datetime.timedelta(days=(365*4))\n",
    "        GLOBAL_SETTINGS[\"opt_start\"] = GLOBAL_SETTINGS[\"test_start\"] - datetime.timedelta(days=(365*2))\n",
    "    \n",
    "    # define bounds for hyperparameters for optimization, here the numbers x are translated into 2^x later\n",
    "    pbounds = {'densesize': (4,8),\n",
    "                'batchsize': (4,9),\n",
    "                'filters': (4,9)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f= bayesOpt_function, #function to optimize\n",
    "        pbounds=pbounds, #parameter bounds\n",
    "        random_state=1, \n",
    "        verbose = 0 # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent, verbose = 2 prints everything\n",
    "        )\n",
    "    \n",
    "    # you may want to conitue by loading an existing optimizer\n",
    "    log_already_available = 0\n",
    "    if os.path.isfile(\"./Results_XAI/logs_CNN_\"+ID+\".json\"):\n",
    "        load_logs(optimizer, logs=[\"./Results_XAI/logs_CNN_\"+ID+\".json\"]);\n",
    "        print(\"\\nExisting optimizer is already aware of {} points. (pp={})\".format(len(optimizer.space),loc))\n",
    "        log_already_available = 1\n",
    "        \n",
    "    # Save progress\n",
    "    logger = newJSONLogger(path=\"./Results_XAI/logs_CNN_\"+ID+\".json\")\n",
    "    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "    \n",
    "    if log_already_available == 0:\n",
    "\n",
    "        optimizer.maximize(init_points=0, n_iter=0)\n",
    "        print(optimizer.max)# uses probe\n",
    "        optimizer.maximize(\n",
    "                init_points=20, #steps of random exploration (random starting points before bayesopt(?))\n",
    "                n_iter=0, # steps of bayesian optimization\n",
    "                acq=\"ei\",# ei  = expected improvmenet (probably the most common acquisition function) \n",
    "                xi=0.05  #  Prefer exploitation (xi=0.0) / Prefer exploration (xi=0.1)\n",
    "                )\n",
    "    \n",
    "    #define counters that control the number of optimization steps\n",
    "    counter1 = 50#80 # min number of steps\n",
    "    counter2 = 1#25 # between min and max, stop after counter2 steps if no improvement\n",
    "    counter3 = 55#200 # max number of steps\n",
    "\n",
    "    #check for progress and which step is the currently best step during optimization\n",
    "    current_step = len(optimizer.res)\n",
    "    beststep = False\n",
    "    step = -1\n",
    "    while not beststep:\n",
    "        step = step + 1\n",
    "        beststep = optimizer.res[step] == optimizer.max\n",
    "\n",
    "    \n",
    "    while current_step < counter1: \n",
    "            current_step = len(optimizer.res)\n",
    "            beststep = False\n",
    "            step = -1\n",
    "            while not beststep:\n",
    "                step = step + 1\n",
    "                beststep = optimizer.res[step] == optimizer.max\n",
    "            print(\"\\nbeststep {}, current step {}\".format(step+1, current_step+1))\n",
    "            optimizer.maximize(\n",
    "                init_points=0, #steps of random exploration / you can also start with counter1 random steps instead of the while loop\n",
    "                n_iter=1, # steps of bayesian optimization\n",
    "                acq=\"ei\",# ei  = expected improvmenet (probably the most common acquisition function) \n",
    "                xi=0.05  #  Prefer exploitation (xi=0.0) / Prefer exploration (xi=0.1)\n",
    "                )\n",
    "\n",
    "    while (step + counter2 > current_step and current_step < counter3): # \n",
    "            current_step = len(optimizer.res)\n",
    "            beststep = False\n",
    "            step = -1\n",
    "            while not beststep:\n",
    "                step = step + 1\n",
    "                beststep = optimizer.res[step] == optimizer.max\n",
    "                \n",
    "            print(\"\\nbeststep {}, current step {}\".format(step+1, current_step+1))\n",
    "            optimizer.maximize(\n",
    "                init_points=0, #steps of random exploration \n",
    "                n_iter=1, # steps of bayesian optimization\n",
    "                acq=\"ei\",# ei  = expected improvmenet (probably the most common acquisition function) \n",
    "                xi=0.05  #  Prefer exploitation (xi=0.0) / Prefer exploration (xi=0.1)\n",
    "                )\n",
    "        \n",
    "    print(\"\\nBEST:\\t{}\".format(optimizer.max))\n",
    "\n",
    "    \n",
    "    #get best values from optimizer\n",
    "    densesize = optimizer.max.get(\"params\").get(\"densesize\")\n",
    "    batchsize = optimizer.max.get(\"params\").get(\"batchsize\")\n",
    "    filters = optimizer.max.get(\"params\").get(\"filters\")\n",
    "        \n",
    "    densesize = 2**int(densesize)\n",
    "    batchsize = 2**int(batchsize)\n",
    "    filters = 2**int(filters)\n",
    "    \n",
    "    GLOBAL_SETTINGS['batch_size'] = batchsize\n",
    "    GLOBAL_SETTINGS['dense_size'] = densesize\n",
    "    GLOBAL_SETTINGS['filters'] = filters\n",
    "    \n",
    "    #run test set simulations\n",
    "    scores, TestData, sim, obs, sim_uncertainty = simulate_testset()\n",
    "    \n",
    "    #plot result\n",
    "    pyplot.figure(figsize=(20,6))\n",
    "    \n",
    "    lb = sim_uncertainty[0]\n",
    "    ub = sim_uncertainty[1]\n",
    "    \n",
    "    pyplot.fill_between(TestData.index, lb,\n",
    "                ub, facecolor = (1,0.7,0,0.5),\n",
    "                label ='90% confidence',linewidth = 1,\n",
    "                edgecolor = (1,0.7,0,0.7))\n",
    "    \n",
    "    pyplot.plot(TestData.index, sim, 'r', label =\"simulated mean\", linewidth = 1.7)\n",
    "    \n",
    "    pyplot.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "    \n",
    "    pyplot.title(\"CNN Model Test: \"+ID, size=17,fontweight = 'bold')\n",
    "    pyplot.ylabel('GWL [m asl]', size=15)\n",
    "    pyplot.xlabel('Date',size=15)\n",
    "    pyplot.legend(fontsize=15,bbox_to_anchor=(1.18, 1),loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "    pyplot.xticks(fontsize=14)\n",
    "    pyplot.yticks(fontsize=14)\n",
    "    \n",
    "    s = \"\"\"NSE = {:.2f}\\nKGE = {:.2f}\\nRÂ²  = {:.2f}\\nRMSE = {:.2f}\\nrRMSE = {:.2f}\n",
    "Bias = {:.2f}\\nrBias = {:.2f}\\n\\nfilters = {:d}\\ndense-size = {:d}\n",
    "batchsize = {:d}\\n\"\"\".format(scores.NSE[0],scores.KGE[0],scores.R2[0],\n",
    "    scores.RMSE[0],scores.rRMSE[0],scores.Bias[0],scores.rBias[0],\n",
    "    filters, densesize,batchsize)\n",
    "    \n",
    "    pyplot.figtext(0.872, 0.18, s, bbox=dict(facecolor='white'),fontsize = 15)\n",
    "    pyplot.savefig('./Results_XAI/Test_'+ID+'_CNN.png', dpi=300,bbox_inches='tight')            \n",
    "    pyplot.show()\n",
    "    \n",
    "    # print scores and results\n",
    "    scores.to_csv('./Results_XAI/scores_'+ID+'.txt',float_format='%.3f',index=False)\n",
    "    printdf = pd.DataFrame(data=np.c_[obs,sim,lb,ub],index=TestData.index)\n",
    "    printdf = printdf.rename(columns={0: 'Obs', 1: 'Sim', 2:'lb:0.05', 3:'ub:95'})\n",
    "    printdf.to_csv('./Results_XAI/results_'+ID+'.txt',sep=';', float_format = '%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
