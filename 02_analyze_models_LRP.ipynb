{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproducability\n",
    "from numpy.random import seed\n",
    "seed(1+347823)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1+63493)\n",
    "\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import innvestigate\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_RM_GW_and_HYRAS_Data(ID):\n",
    "    pathGW = \"./01_GWdata/\"\n",
    "    pathHYRAS = \"./00_HYRAS/\"\n",
    "\n",
    "    GWData = pd.read_csv(pathGW+ID+'_GW-Data.csv', \n",
    "                          parse_dates=['Date'],index_col=0, dayfirst = True, \n",
    "                          decimal = '.', sep=',')\n",
    "    HYRASData = pd.read_csv(pathHYRAS+ID+'_Hyras_weekly.csv',\n",
    "                            parse_dates=['Date'],index_col=0, dayfirst = True,\n",
    "                            decimal = '.', sep=',')\n",
    "    data = pd.merge(GWData, HYRASData, how='inner', left_index = True, right_index = True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_data(data, GLOBAL_SETTINGS):\n",
    "    '''\n",
    "    split data in four parts: training, early stopping, optimization, and testing\n",
    "    strict dates for testing and optimization, relative portions for training and early stopping of the rest\n",
    "    according to the input sequence length the parts do overlap (\"_ext\"), so the model can make use of the full data\n",
    "    '''\n",
    "    dataset = data[(data.index < GLOBAL_SETTINGS[\"test_start\"])]\n",
    "    dataset2 = data[(data.index < GLOBAL_SETTINGS[\"opt_start\"])]\n",
    "    \n",
    "    TrainingData = dataset2[0:round(0.9 * len(dataset2))]\n",
    "    StopData = dataset2[round(0.9 * len(dataset2))+1:]\n",
    "    StopData_ext = dataset2[round(0.9 * len(dataset2))+1-GLOBAL_SETTINGS[\"seq_length\"]:] \n",
    "    OptData = data[(data.index >= GLOBAL_SETTINGS[\"opt_start\"]) & (data.index < GLOBAL_SETTINGS[\"test_start\"])] \n",
    "    OptData_ext = pd.concat([dataset2.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], OptData], axis=0)                                             \n",
    "\n",
    "    TestData = data[(data.index >= GLOBAL_SETTINGS[\"test_start\"]) & (data.index <= GLOBAL_SETTINGS[\"test_end\"])] \n",
    "    TestData_ext = pd.concat([dataset.iloc[-GLOBAL_SETTINGS[\"seq_length\"]:], TestData], axis=0)                                             \n",
    "\n",
    "    return TrainingData, StopData, StopData_ext, OptData, OptData_ext, TestData, TestData_ext\n",
    "\n",
    "def to_supervised(data, GLOBAL_SETTINGS):\n",
    "    '''establish sequence to value data format, function based on code from machinelearningmastery.com'''\n",
    "    X, Y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(data)):\n",
    "        # find the end of this pattern\n",
    "        end_idx = i + GLOBAL_SETTINGS[\"seq_length\"]\n",
    "        # check if we are beyond the dataset\n",
    "        if end_idx >= len(data):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = data[i:end_idx, 1:], data[end_idx, 0]\n",
    "        X.append(seq_x)\n",
    "        Y.append(seq_y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def bayesOpt_function():\n",
    "    '''can be empty here, just needed for the loading logs module'''\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "well_list = pd.read_csv(\"./locations.csv\",sep=';',header = 0,encoding = 'Latin1')\n",
    "\n",
    "for loc in [0]:#range(well_list.shape[0]): # loop all locations\n",
    "    seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "    \n",
    "    ID = well_list.alias[loc]\n",
    "    print(ID+\": \"+str(loc))\n",
    "    \n",
    "    # define bounds for hyperparameters for optimization, here the numbers x are translated into 2^x later\n",
    "    pbounds = { 'densesize': (4,8),\n",
    "                'batchsize': (4,9),\n",
    "                'filters': (4,9)\n",
    "                }\n",
    "    \n",
    "    BayesOptimizer = BayesianOptimization(\n",
    "        f= bayesOpt_function, #Funktion die optimiert wird\n",
    "        pbounds=pbounds, #Wertebereiche in denen optimiert wird\n",
    "        random_state=1, \n",
    "        verbose = 0 # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent, verbose = 2 prints everything\n",
    "        )\n",
    "    \n",
    "    # #load existing optimizer\n",
    "    load_logs(BayesOptimizer, logs=[\"./Results_XAI/logs_CNN_\"+ID+\".json\"])\n",
    "    \n",
    "    #get best values from optimizer\n",
    "    densesize = BayesOptimizer.max.get(\"params\").get(\"densesize\")\n",
    "    batchsize = BayesOptimizer.max.get(\"params\").get(\"batchsize\")\n",
    "    filters = BayesOptimizer.max.get(\"params\").get(\"filters\")\n",
    "    \n",
    "    densesize = 2**int(densesize)\n",
    "    batchsize = 2**int(batchsize)\n",
    "    filters = 2**int(filters)\n",
    "    seqlength = 52\n",
    "\n",
    "    #%%\n",
    "    GLOBAL_SETTINGS = {\n",
    "        'batch_size': batchsize, # best value from optimization\n",
    "        'dense_size': densesize, # best value from optimization\n",
    "        'filters': filters, # best value from optimization\n",
    "        'clip_norm': True,\n",
    "        'epochs': 3, #max number of training epochs to allow\n",
    "        'patience': 30, # early stopping patience\n",
    "        'learning_rate': 1e-3,\n",
    "        'seq_length': seqlength, #length of the data input sequence for the model (here: 52 weeks = 1 year)\n",
    "        'kernel_size': 3,\n",
    "        'opt_start': pd.to_datetime('01012015', format='%d%m%Y'), \n",
    "        'test_start': pd.to_datetime('01012017', format='%d%m%Y'),\n",
    "        'test_end': pd.to_datetime('31122020', format='%d%m%Y')\n",
    "        }\n",
    "        \n",
    "    ## load data\n",
    "    data = load_RM_GW_and_HYRAS_Data(ID)\n",
    "    \n",
    "    if GLOBAL_SETTINGS[\"test_end\"] > data.index[-1]:\n",
    "        GLOBAL_SETTINGS[\"test_end\"] = data.index[-1]\n",
    "        GLOBAL_SETTINGS[\"test_start\"] = GLOBAL_SETTINGS[\"test_end\"] - datetime.timedelta(days=(365*4))\n",
    "        GLOBAL_SETTINGS[\"opt_start\"] = GLOBAL_SETTINGS[\"test_start\"] - datetime.timedelta(days=(365*2))\n",
    "        \n",
    "    #scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_gwl.fit(pd.DataFrame(data['GWL']))\n",
    "    data_n = pd.DataFrame(scaler.fit_transform(data), index=data.index, columns=data.columns)\n",
    "\n",
    "    #split data\n",
    "    TrainingData, StopData, StopData_ext, OptData, OptData_ext, TestData, TestData_ext = split_data(data, GLOBAL_SETTINGS)\n",
    "    TrainingData_n, StopData_n, StopData_ext_n, OptData_n, OptData_ext_n, TestData_n, TestData_ext_n = split_data(data_n, GLOBAL_SETTINGS)\n",
    "    \n",
    "    #sequence data\n",
    "    X_train, Y_train = to_supervised(TrainingData_n.values, GLOBAL_SETTINGS)\n",
    "    X_stop, Y_stop = to_supervised(StopData_ext_n.values, GLOBAL_SETTINGS) \n",
    "    X_opt, Y_opt = to_supervised(OptData_ext_n.values, GLOBAL_SETTINGS)\n",
    "    X_test, Y_test = to_supervised(TestData_ext_n.values, GLOBAL_SETTINGS) \n",
    "\n",
    "    #build and train model with different initializations\n",
    "    analysis_list=[]\n",
    "    y_list=[]\n",
    "    inimax = 20\n",
    "    model_path = './Results_XAI/models/'+ID+\"/\"\n",
    "    \n",
    "    for ini in range(inimax):\n",
    "        print('load: '+str(ini))\n",
    "        model = tf.keras.models.load_model(model_path+str(ini))\n",
    "        \n",
    "        y = model.predict(X_test)\n",
    "        y = scaler_gwl.inverse_transform(y)\n",
    "        \n",
    "        obs = np.array(TestData.GWL).reshape(-1,1)\n",
    "        NSE = 1 - np.sum((obs-y)**2) / np.sum((obs-np.mean(obs))**2)\n",
    "        \n",
    "        #plot to see if any ensemble member shows unwanted behavior\n",
    "        pyplot.figure(figsize=(3,1))\n",
    "        pyplot.plot(TestData.index,TestData.GWL,color='k',label='observed')\n",
    "        pyplot.plot(TestData.index,y,color='r',label='simulated')\n",
    "        pyplot.legend()\n",
    "        pyplot.title('NSE {} - '.format(str(np.round(NSE,2)))+ID+' - ini '+str(ini))\n",
    "        pyplot.xlim([TestData.index[0],TestData.index[-1]])#\n",
    "        pyplot.show()\n",
    "        \n",
    "        #%%apply LRP analysis\n",
    "        X_full, Y_full = to_supervised(data_n.values, GLOBAL_SETTINGS)\n",
    "        \n",
    "        X_investigate = X_full # relevant data for LRP analysis\n",
    "        data_investigate = data.iloc[seqlength:,:].copy()\n",
    "        data_investigate_ext = data.copy()\n",
    "        method = 'lrp.z' \n",
    "        analyzer = innvestigate.create_analyzer(method, model,reverse_verbose=True , neuron_selection_mode='all')\n",
    "        analysis = analyzer.analyze(X_investigate)\n",
    "        analysis_list.append(analysis)\n",
    "\n",
    "        # make and save all predictions on the data\n",
    "        y = model.predict(X_investigate)\n",
    "        y = scaler_gwl.inverse_transform(y)\n",
    "        y_list.append(y)\n",
    "   \n",
    "    #plot and save model fit on full time series\n",
    "    sim_members = np.zeros(shape = (y_list[0].shape[0],inimax))\n",
    "    for i in range(inimax):\n",
    "        sim_members[:,i] = y_list[i].reshape(-1,)\n",
    "    sim_mean = np.nanmean(sim_members,axis = 1)\n",
    "    sim_uncertainty = [np.quantile(sim_members, 0.05, axis=1),np.quantile(sim_members, 0.95, axis=1)]\n",
    "    \n",
    "    pyplot.figure(figsize=(20,5))\n",
    "    lb = sim_uncertainty[0]\n",
    "    ub = sim_uncertainty[1]\n",
    "    pyplot.fill_between(data_investigate.index, lb,\n",
    "                ub, facecolor = (1,0.7,0,0.5),\n",
    "                label ='90% confidence',linewidth = 1,\n",
    "                edgecolor = (1,0.7,0,0.7))\n",
    "    \n",
    "    pyplot.plot(data_investigate.index, sim_mean, 'r', label =\"simulated mean\", linewidth = 1.7)\n",
    "    pyplot.plot(data_investigate.index, data_investigate['GWL'], 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "    pyplot.title(ID, size=17,fontweight = 'bold')\n",
    "    pyplot.ylabel('GWL [m asl]', size=15)\n",
    "    pyplot.xlabel('Date',size=15)\n",
    "    pyplot.legend(fontsize=15,fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "    pyplot.xticks(fontsize=16)\n",
    "    pyplot.yticks(fontsize=16)\n",
    "    pyplot.savefig('./Results_XAI/Full_'+ID+'_CNN.png', dpi=300,bbox_inches='tight')            \n",
    "    pyplot.show()\n",
    "    \n",
    "    #dump results in pickle file\n",
    "    analysis = analysis_list[0]\n",
    "    for i in range(1,inimax):\n",
    "        analysis = analysis+analysis_list[i]\n",
    "    analysis = analysis/inimax\n",
    "    \n",
    "    dump = {}\n",
    "    dump['data_investigate'] = data_investigate\n",
    "    dump['data_investigate_ext'] = data_investigate_ext\n",
    "    dump['analysis_list'] = analysis_list\n",
    "    dump['analysis_mean'] = analysis\n",
    "    dump['simulations'] = y_list\n",
    "    file = './Results_XAI/analysis_'+ID+'_'+method.replace('.','')+'.pickle'\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(dump, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
